# CS-144
Network Structures and Economics

My crawler used a breadth first search selection policy. It made sure to avoid any pages with “?” in their URL (dynamic content) and any pages that did not have “caltech.edu” in their URL, as this would be leaving the domain we were attempting to crawl. The breadth-first approach allowed for a broad, general view of the graph. By using BFS, we got to see nodes splitting off in multiple different directions, rather than getting stuck in a long pathway and really only adventuring that one thoroughly. The reasoning behind this was that the early child nodes are likely more important than the later child nodes. Also, we removed all leaf nodes that were not in the Caltech domain. This way, we get a better representation of the Caltech graph. If people want to access a page, it shouldn’t be very hard to access, so it will be close to the top. The major downside of this approach is that it would take a very long time to find the true maximal diameter. Since we add all child nodes to the queue, we don’t go down one particular pathway especially quickly, so the maximal diameter is likely somewhat skewed.
